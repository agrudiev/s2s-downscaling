# Experiment
experiment_name: diff_nolambda

# Data from preprocessing
max_len: 15000 # maximum number of images to load
validation_ratio: 0.0 # this will get a set of random images by default

# Network to be used by the CM [diffusers, homemade, udffdb, conditional]
network: diffusers
main_variables: [precip]
conditional_variables: # [hours, months]

# Network configuration
dropout: 0.3

# Distance [l1, l2, lpips]
distance: l1

# Loss weighting
loss_weight: True # whether or nor to use the lambda schedule

# Optimizer
optimizer: radam

# Hyperparameters (close to Song et al. 2023 for LSUN 256x256)
learning_rate: 1e-6
batch_size: 4
min_noise: 0.002
imin: 1 # minimum noise step
max_noise: 80.0
imax: 1280 # maximum noise step
mu0: 0.9 # ema decay rating at the beginning of training
s0: 10 # initial discretization steps
s1: 1280 # target discretization steps
discretization_steps: schedule # [float, schedule, schedule_improved]
mu: 0.0 # EMA decay rating (can be a "schedule")
epochs: 100 # K
EMA_decay_rate: 0.9999 # EMA decay rating for the online network

# Normalizations
log_transform: True # log-transform the data before training
norm_mean: 0.0
norm_std: 0.5

# Logging and checkpointing
log_each: 500
checkpoint_each: 1 # checkpoint each #number of epochs

