{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIW-If53CiPL"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "czOvyn7HCbgV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/google-research/swirl-dynamics.git@main\n",
            "  Cloning https://github.com/google-research/swirl-dynamics.git (to revision main) to /tmp/45333636/pip-req-build-326g0q7o\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/swirl-dynamics.git /tmp/45333636/pip-req-build-326g0q7o\n",
            "  Resolved https://github.com/google-research/swirl-dynamics.git to commit fdf118d5fedfb2644cd9930ec6f210b924d7388f\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: chex in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (0.1.86)\n",
            "Collecting clu (from swirl_dynamics==0.0.1)\n",
            "  Using cached clu-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: etils in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (1.8.0)\n",
            "Requirement already satisfied: flax in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (0.8.3)\n",
            "Requirement already satisfied: h5py in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (3.11.0)\n",
            "Collecting grain-nightly (from swirl_dynamics==0.0.1)\n",
            "  Using cached grain_nightly-0.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting gin-config (from swirl_dynamics==0.0.1)\n",
            "  Using cached gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: jax in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (0.4.28)\n",
            "Requirement already satisfied: matplotlib in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (3.8.4)\n",
            "Requirement already satisfied: numpy in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: optax in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (0.4.4)\n",
            "Collecting tensorflow (from swirl_dynamics==0.0.1)\n",
            "  Downloading tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting tqdm (from swirl_dynamics==0.0.1)\n",
            "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: xarray in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from swirl_dynamics==0.0.1) (2023.6.0)\n",
            "Collecting xarray_tensorstore (from swirl_dynamics==0.0.1)\n",
            "  Using cached xarray_tensorstore-0.1.1-py3-none-any.whl.metadata (556 bytes)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from chex->swirl_dynamics==0.0.1) (4.9.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from chex->swirl_dynamics==0.0.1) (0.4.28)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from chex->swirl_dynamics==0.0.1) (0.12.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from jax->swirl_dynamics==0.0.1) (0.3.1)\n",
            "Requirement already satisfied: opt-einsum in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from jax->swirl_dynamics==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from jax->swirl_dynamics==0.0.1) (1.13.0)\n",
            "Collecting ml-collections (from clu->swirl_dynamics==0.0.1)\n",
            "  Using cached ml_collections-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from clu->swirl_dynamics==0.0.1) (23.2)\n",
            "Collecting wrapt (from clu->swirl_dynamics==0.0.1)\n",
            "  Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: msgpack in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from flax->swirl_dynamics==0.0.1) (1.0.3)\n",
            "Requirement already satisfied: tensorstore in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from flax->swirl_dynamics==0.0.1) (0.1.56)\n",
            "Requirement already satisfied: rich>=11.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from flax->swirl_dynamics==0.0.1) (13.3.5)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from flax->swirl_dynamics==0.0.1) (6.0.1)\n",
            "Collecting array-record (from grain-nightly->swirl_dynamics==0.0.1)\n",
            "  Using cached array_record-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (699 bytes)\n",
            "Collecting cloudpickle (from grain-nightly->swirl_dynamics==0.0.1)\n",
            "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: dm-tree in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from grain-nightly->swirl_dynamics==0.0.1) (0.1.7)\n",
            "Collecting jaxtyping (from grain-nightly->swirl_dynamics==0.0.1)\n",
            "  Using cached jaxtyping-0.2.31-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting more-itertools>=9.1.0 (from grain-nightly->swirl_dynamics==0.0.1)\n",
            "  Using cached more_itertools-10.3.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from matplotlib->swirl_dynamics==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from matplotlib->swirl_dynamics==0.0.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from matplotlib->swirl_dynamics==0.0.1) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from matplotlib->swirl_dynamics==0.0.1) (1.4.4)\n",
            "Requirement already satisfied: pillow>=8 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from matplotlib->swirl_dynamics==0.0.1) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from matplotlib->swirl_dynamics==0.0.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from matplotlib->swirl_dynamics==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: nest_asyncio in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from orbax-checkpoint->swirl_dynamics==0.0.1) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from orbax-checkpoint->swirl_dynamics==0.0.1) (5.26.1)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting protobuf (from orbax-checkpoint->swirl_dynamics==0.0.1)\n",
            "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow->swirl_dynamics==0.0.1) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow->swirl_dynamics==0.0.1) (68.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow->swirl_dynamics==0.0.1) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached grpcio-1.65.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.2.0 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pandas>=1.4 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from xarray->swirl_dynamics==0.0.1) (2.2.1)\n",
            "Collecting zarr (from xarray_tensorstore->swirl_dynamics==0.0.1)\n",
            "  Using cached zarr-2.18.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->swirl_dynamics==0.0.1) (0.43.0)\n",
            "Collecting namex (from keras>=3.2.0->tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting optree (from keras>=3.2.0->tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached optree-0.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from pandas>=1.4->xarray->swirl_dynamics==0.0.1) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from pandas>=1.4->xarray->swirl_dynamics==0.0.1) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->swirl_dynamics==0.0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->swirl_dynamics==0.0.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->swirl_dynamics==0.0.1) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow->swirl_dynamics==0.0.1) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from rich>=11.1->flax->swirl_dynamics==0.0.1) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from rich>=11.1->flax->swirl_dynamics==0.0.1) (2.15.1)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow->swirl_dynamics==0.0.1)\n",
            "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: fsspec in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->swirl_dynamics==0.0.1) (2024.3.1)\n",
            "Requirement already satisfied: importlib_resources in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->swirl_dynamics==0.0.1) (6.4.0)\n",
            "Requirement already satisfied: zipp in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->swirl_dynamics==0.0.1) (3.17.0)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping->grain-nightly->swirl_dynamics==0.0.1)\n",
            "  Using cached typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting contextlib2 (from ml-collections->clu->swirl_dynamics==0.0.1)\n",
            "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting asciitree (from zarr->xarray_tensorstore->swirl_dynamics==0.0.1)\n",
            "  Using cached asciitree-0.3.3-py3-none-any.whl\n",
            "Collecting numcodecs>=0.10.0 (from zarr->xarray_tensorstore->swirl_dynamics==0.0.1)\n",
            "  Using cached numcodecs-0.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting fasteners (from zarr->xarray_tensorstore->swirl_dynamics==0.0.1)\n",
            "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax->swirl_dynamics==0.0.1) (0.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->swirl_dynamics==0.0.1) (2.1.5)\n",
            "Using cached clu-0.0.12-py3-none-any.whl (101 kB)\n",
            "Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
            "Using cached grain_nightly-0.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
            "Downloading tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Using cached xarray_tensorstore-0.1.1-py3-none-any.whl (8.6 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.65.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "Using cached keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "Using cached more_itertools-10.3.0-py3-none-any.whl (59 kB)\n",
            "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "Using cached array_record-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Using cached jaxtyping-0.2.31-py3-none-any.whl (41 kB)\n",
            "Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Using cached zarr-2.18.2-py3-none-any.whl (210 kB)\n",
            "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "Using cached numcodecs-0.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
            "Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Using cached optree-0.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
            "Building wheels for collected packages: swirl_dynamics\n",
            "  Building wheel for swirl_dynamics (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for swirl_dynamics: filename=swirl_dynamics-0.0.1-py3-none-any.whl size=318287 sha256=b5bb94559b3cd2d05e9a1bec9c8226f9589cb39b19fd67a280a3700890280199\n",
            "  Stored in directory: /tmp/45333636/pip-ephem-wheel-cache-1crqairh/wheels/14/e6/4b/60ef35a9f652b16b911a36954f92f024cc7b9c207d1553532c\n",
            "Successfully built swirl_dynamics\n",
            "Installing collected packages: namex, libclang, gin-config, flatbuffers, asciitree, wrapt, werkzeug, typeguard, tqdm, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, numcodecs, more-itertools, markdown, grpcio, google-pasta, gast, fasteners, contextlib2, cloudpickle, astunparse, zarr, tensorboard, ml-collections, jaxtyping, keras, array-record, xarray_tensorstore, tensorflow, grain-nightly, clu, swirl_dynamics\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.26.1\n",
            "    Uninstalling protobuf-5.26.1:\n",
            "      Successfully uninstalled protobuf-5.26.1\n",
            "Successfully installed array-record-0.5.1 asciitree-0.3.3 astunparse-1.6.3 cloudpickle-3.0.0 clu-0.0.12 contextlib2-21.6.0 fasteners-0.19 flatbuffers-24.3.25 gast-0.6.0 gin-config-0.5.0 google-pasta-0.2.0 grain-nightly-0.0.7 grpcio-1.65.0 jaxtyping-0.2.31 keras-3.4.1 libclang-18.1.1 markdown-3.6 ml-collections-0.1.1 more-itertools-10.3.0 namex-0.0.8 numcodecs-0.12.1 optree-0.12.1 protobuf-4.25.3 swirl_dynamics-0.0.1 tensorboard-2.17.0 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.4.0 tqdm-4.66.4 typeguard-2.13.3 werkzeug-3.0.3 wrapt-1.16.0 xarray_tensorstore-0.1.1 zarr-2.18.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/google-research/swirl-dynamics.git@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p2DJBuEdCpFc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-datasets\n",
            "  Using cached tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: absl-py in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (2.1.0)\n",
            "Requirement already satisfied: click in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (0.1.7)\n",
            "Collecting immutabledict (from tensorflow-datasets)\n",
            "  Using cached immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (1.26.4)\n",
            "Collecting promise (from tensorflow-datasets)\n",
            "  Using cached promise-2.3-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.20 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (4.25.3)\n",
            "Requirement already satisfied: psutil in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (5.9.0)\n",
            "Collecting pyarrow (from tensorflow-datasets)\n",
            "  Using cached pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (2.32.3)\n",
            "Collecting simple-parsing (from tensorflow-datasets)\n",
            "  Using cached simple_parsing-0.1.5-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
            "  Using cached tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: termcolor in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (2.4.0)\n",
            "Collecting toml (from tensorflow-datasets)\n",
            "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tqdm in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (4.66.4)\n",
            "Requirement already satisfied: wrapt in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from tensorflow-datasets) (0.5.1)\n",
            "Collecting etils>=1.9.1 (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
            "  Using cached etils-1.9.2-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: fsspec in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2024.3.1)\n",
            "Requirement already satisfied: importlib_resources in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.4.0)\n",
            "Requirement already satisfied: typing_extensions in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.9.0)\n",
            "Requirement already satisfied: zipp in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.6.2)\n",
            "Requirement already satisfied: six in /work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Collecting docstring-parser~=0.15 (from simple-parsing->tensorflow-datasets)\n",
            "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
            "  Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Using cached tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
            "Using cached etils-1.9.2-py3-none-any.whl (161 kB)\n",
            "Using cached immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
            "Using cached pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "Using cached simple_parsing-0.1.5-py3-none-any.whl (113 kB)\n",
            "Using cached tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
            "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
            "Installing collected packages: toml, pyarrow, promise, immutabledict, googleapis-common-protos, etils, docstring-parser, tensorflow-metadata, simple-parsing, tensorflow-datasets\n",
            "  Attempting uninstall: etils\n",
            "    Found existing installation: etils 1.8.0\n",
            "    Uninstalling etils-1.8.0:\n",
            "      Successfully uninstalled etils-1.8.0\n",
            "Successfully installed docstring-parser-0.16 etils-1.9.2 googleapis-common-protos-1.63.2 immutabledict-4.2.0 promise-2.3 pyarrow-16.1.0 simple-parsing-0.1.5 tensorflow-datasets-4.9.6 tensorflow-metadata-1.15.0 toml-0.10.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP6GQNwnCrwz"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZDKhSAGaCrk2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-11 22:46:53.471908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-11 22:46:53.691968: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-11 22:46:53.777865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-11 22:46:55.445603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/work/FAC/FGSE/IDYST/tbeucler/downscaling/mlima/miniconda3/envs/swirl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "\n",
        "from clu import metric_writers\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from swirl_dynamics import templates\n",
        "from swirl_dynamics.lib import diffusion as dfn_lib\n",
        "from swirl_dynamics.lib import solvers as solver_lib\n",
        "from swirl_dynamics.projects import probabilistic_diffusion as dfn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ylBqVdcCvcz"
      },
      "source": [
        "## Example I - Unconditional diffusion model with guidance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVzRTpB5Dgm2"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THwrp-2UD2iS"
      },
      "source": [
        "First we need a dataset containing samples whose distribution is to be modeled by the diffusion model. For demonstration purpose, we use the MNIST dataset provided by TensorFlow Datasets.\n",
        "\n",
        "Our code setup accepts any Python iterable objects to be used as dataloaders. The expectation is that they should continuously yield a dictionary with a field named `x` whose corresponding value is a numpy array with shape `(batch, *spatial_dims, channels)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRf3AcadCvKj"
      },
      "outputs": [],
      "source": [
        "def get_mnist_dataset(split: str, batch_size: int):\n",
        "  ds = tfds.load(\"mnist\", split=split)\n",
        "  ds = ds.map(\n",
        "      # Change field name from \"image\" to \"x\" (required by `DenoisingModel`)\n",
        "      # and normalize the value to [0, 1].\n",
        "      lambda x: {\"x\": tf.cast(x[\"image\"], tf.float32) / 255.0}\n",
        "  )\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  ds = ds.as_numpy_iterator()\n",
        "  return ds\n",
        "\n",
        "# The standard deviation of the normalized dataset.\n",
        "# This is useful for determining the diffusion scheme and preconditioning\n",
        "# of the neural network parametrization.\n",
        "DATA_STD = 0.31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5tZdk5eEQhh"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA4hHUeHEVoE"
      },
      "source": [
        "Next let's define the U-Net backbone. The \"Preconditioning\" is to ensure that the inputs and outputs of the network are roughly standardized (for more details, see Appendix B.6. in [this paper](https://arxiv.org/abs/2206.00364))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE1-wf7aETEH"
      },
      "outputs": [],
      "source": [
        "denoiser_model = dfn_lib.PreconditionedDenoiserUNet(\n",
        "    out_channels=1,\n",
        "    num_channels=(64, 128),\n",
        "    downsample_ratio=(2, 2),\n",
        "    num_blocks=4,\n",
        "    noise_embed_dim=128,\n",
        "    padding=\"SAME\",\n",
        "    use_attention=True,\n",
        "    use_position_encoding=True,\n",
        "    num_heads=8,\n",
        "    sigma_data=DATA_STD,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htjx7TxAEsKW"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5i7oh2WFFU0"
      },
      "source": [
        "For diffusion model training, the above-defined U-Net backbone serves as a denoiser, which takes as input a batch of (isotropic Gaussian noise) corrupted samples and outputs its best guess for what the uncorrupted image would be.\n",
        "\n",
        "Besides the backbone architecture, we also need to specify how to sample the noise levels (i.e. standard deviations) used to corrupt the samples and the weighting for each noise level in the loss function (for available options and configurations, see [`swirl_dynamics.lib.diffusion.diffusion`](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/lib/diffusion/diffusion.py)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l52TFsPEEp5u"
      },
      "outputs": [],
      "source": [
        "diffusion_scheme = dfn_lib.Diffusion.create_variance_exploding(\n",
        "    sigma=dfn_lib.tangent_noise_schedule(),\n",
        "    data_std=DATA_STD,\n",
        ")\n",
        "\n",
        "model = dfn.DenoisingModel(\n",
        "    # `input_shape` must agree with the expected sample shape (without the batch\n",
        "    # dimension), which in this case is simply the dimensions of a single MNIST\n",
        "    # sample.\n",
        "    input_shape=(28, 28, 1),\n",
        "    denoiser=denoiser_model,\n",
        "    noise_sampling=dfn_lib.log_uniform_sampling(\n",
        "        diffusion_scheme, clip_min=1e-4, uniform_grid=True,\n",
        "    ),\n",
        "    noise_weighting=dfn_lib.edm_weighting(data_std=DATA_STD),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76a8RhUpFLoe"
      },
      "source": [
        "We are now ready to define the learning parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8fl_R1gEp36"
      },
      "outputs": [],
      "source": [
        "# !rm -R -f $workdir  # optional: clear the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4DAOL_xEp1k"
      },
      "outputs": [],
      "source": [
        "num_train_steps = 100_000  #@param\n",
        "workdir = \"/tmp/diffusion_demo_mnist\"  #@param\n",
        "train_batch_size = 32  #@param\n",
        "eval_batch_size = 32  #@param\n",
        "initial_lr = 0.0  #@param\n",
        "peak_lr = 1e-4  #@param\n",
        "warmup_steps = 1000  #@param\n",
        "end_lr = 1e-6  #@param\n",
        "ema_decay = 0.999  #@param\n",
        "ckpt_interval = 1000  #@param\n",
        "max_ckpt_to_keep = 5  #@param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr-be7CiFRhp"
      },
      "source": [
        "To start training, we first need to initialize the trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7E4WZUFEpzv"
      },
      "outputs": [],
      "source": [
        "# NOTE: use `trainers.DistributedDenoisingTrainer` for multi-device\n",
        "# training with data parallelism.\n",
        "trainer = dfn.DenoisingTrainer(\n",
        "    model=model,\n",
        "    rng=jax.random.PRNGKey(888),\n",
        "    optimizer=optax.adam(\n",
        "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
        "            init_value=initial_lr,\n",
        "            peak_value=peak_lr,\n",
        "            warmup_steps=warmup_steps,\n",
        "            decay_steps=num_train_steps,\n",
        "            end_value=end_lr,\n",
        "        ),\n",
        "    ),\n",
        "    # We keep track of an exponential moving average of the model parameters\n",
        "    # over training steps. This alleviates the \"color-shift\" problems known to\n",
        "    # exist in the diffusion models.\n",
        "    ema_decay=ema_decay,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTQf9w1NFgp7"
      },
      "source": [
        "Now we are ready to kick start training. A couple of \"callbacks\" are passed to assist with monitoring and checkpointing.\n",
        "\n",
        "The first step will be a little slow as Jax needs to JIT compile the step function (the same goes for the first step where evaluation is performed). Fortunately, steps after that should continue much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMQbydfdEpxQ"
      },
      "outputs": [],
      "source": [
        "templates.run_train(\n",
        "    train_dataloader=get_mnist_dataset(\n",
        "        split=\"train[:75%]\", batch_size=train_batch_size\n",
        "    ),\n",
        "    trainer=trainer,\n",
        "    workdir=workdir,\n",
        "    total_train_steps=num_train_steps,\n",
        "    metric_writer=metric_writers.create_default_writer(\n",
        "        workdir, asynchronous=False\n",
        "    ),\n",
        "    metric_aggregation_steps=100,\n",
        "    eval_dataloader=get_mnist_dataset(\n",
        "        split=\"train[75%:]\", batch_size=eval_batch_size\n",
        "    ),\n",
        "    eval_every_steps = 1000,\n",
        "    num_batches_per_eval = 2,\n",
        "    callbacks=(\n",
        "        # This callback displays the training progress in a tqdm bar\n",
        "        templates.TqdmProgressBar(\n",
        "            total_train_steps=num_train_steps,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "        # This callback saves model checkpoint periodically\n",
        "        templates.TrainStateCheckpoint(\n",
        "            base_dir=workdir,\n",
        "            options=ocp.CheckpointManagerOptions(\n",
        "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJDsoPMcFnJ0"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGPHjX8bFqrX"
      },
      "source": [
        "#### Unconditional generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUHa793NF0AS"
      },
      "source": [
        "After training is complete, the trained denoiser may be used to generate unconditional samples.\n",
        "\n",
        "First, let's restore the model from checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-CbKOV9EpvI"
      },
      "outputs": [],
      "source": [
        "# Restore train state from checkpoint. By default, the move recently saved\n",
        "# checkpoint is restored. Alternatively, one can directly use\n",
        "# `trainer.train_state` if continuing from the training section above.\n",
        "trained_state = dfn.DenoisingModelTrainState.restore_from_orbax_ckpt(\n",
        "    f\"{workdir}/checkpoints\", step=None\n",
        ")\n",
        "# Construct the inference function\n",
        "denoise_fn = dfn.DenoisingTrainer.inference_fn_from_state_dict(\n",
        "    trained_state, use_ema=True, denoiser=denoiser_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT2c5b6FF9jP"
      },
      "source": [
        "Diffusion samples are generated by plugging the trained denoising function in a stochastic differential equation (parametrized by the diffusion scheme) and solving it backwards in time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiOByVtjEpUs"
      },
      "outputs": [],
      "source": [
        "sampler = dfn_lib.SdeSampler(\n",
        "    input_shape=(28, 28, 1),\n",
        "    integrator=solver_lib.EulerMaruyama(),\n",
        "    tspan=dfn_lib.edm_noise_decay(\n",
        "        diffusion_scheme, rho=7, num_steps=256, end_sigma=1e-3,\n",
        "    ),\n",
        "    scheme=diffusion_scheme,\n",
        "    denoise_fn=denoise_fn,\n",
        "    guidance_transforms=(),\n",
        "    apply_denoise_at_end=True,\n",
        "    return_full_paths=False,  # Set to `True` if the full sampling paths are needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA26cPp0GBwC"
      },
      "source": [
        "The sampler may be run by calling its `.generate()` function. Optionally, we may JIT compile this function so that it runs faster if repeatedly called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7yaiI7NF_61"
      },
      "outputs": [],
      "source": [
        "generate = jax.jit(sampler.generate, static_argnames=('num_samples',))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qUxgUaKGENj"
      },
      "outputs": [],
      "source": [
        "samples = generate(\n",
        "    rng=jax.random.PRNGKey(8888), num_samples=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjpNNHtGH9c"
      },
      "source": [
        "Visualize the generated samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thhRfIe5GELQ"
      },
      "outputs": [],
      "source": [
        "# Plot generated samples\n",
        "fig, ax = plt.subplots(1, 4, figsize=(8, 2))\n",
        "for i in range(4):\n",
        "  im = ax[i].imshow(samples[i, :, :, 0] * 255, cmap=\"gray\", vmin=0, vmax=255)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cr6J3hfGLi7"
      },
      "source": [
        "#### Guided generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8-3Hr5IGReH"
      },
      "source": [
        "To achieve 'guided' generation, we can modify a trained denoising function and tailor it to produce samples with specific desired characteristics. For instance, in an out-filling task where the goal is to generate full images from a given patch, we can guide the denoiser to create samples whose crops at certain positions precisely align with the provided patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7P4li0qGEI4"
      },
      "outputs": [],
      "source": [
        "guidance_fn = dfn_lib.InfillFromSlices(\n",
        "    # This specifies location of the guide input using python slices.\n",
        "    # Here it implies that the guide input corresponds the 7x7 patch in the\n",
        "    # center of the image.\n",
        "    slices=(slice(None), slice(11, 18), slice(11, 18)),\n",
        "\n",
        "    # This is a parameter that controls how \"hard\" the denoiser pushes for\n",
        "    # the conditioning to be satisfied. It is a tradeoff between strictness of\n",
        "    # constraint satisfication and diversity in the generated samples.\n",
        "    guide_strength=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D5zBynzGUdY"
      },
      "source": [
        "This transform function is passed through the `guidance_transforms` arg of the sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_kJnXqjGEGz"
      },
      "outputs": [],
      "source": [
        "guided_sampler = dfn_lib.SdeSampler(\n",
        "    input_shape=(28, 28, 1),\n",
        "    integrator=solver_lib.EulerMaruyama(),\n",
        "    tspan=dfn_lib.edm_noise_decay(\n",
        "        diffusion_scheme, rho=7, num_steps=256, end_sigma=1e-3,\n",
        "    ),\n",
        "    scheme=diffusion_scheme,\n",
        "    denoise_fn=denoise_fn,\n",
        "    guidance_transforms=(guidance_fn,),\n",
        "    apply_denoise_at_end=True,\n",
        "    return_full_paths=False,\n",
        ")\n",
        "\n",
        "guided_generate = jax.jit(guided_sampler.generate, static_argnames=('num_samples',))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcDPFQ9PGX9-"
      },
      "source": [
        "We construct an example guidance input from a real sample and use it to guide the sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Uap93nGEEx"
      },
      "outputs": [],
      "source": [
        "test_ds = get_mnist_dataset(split=\"test\", batch_size=1)\n",
        "test_example = next(iter(test_ds))[\"x\"]\n",
        "example_guidance_inputs = {'observed_slices': test_example[:, 11:18, 11:18]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdUrseQOGECo"
      },
      "outputs": [],
      "source": [
        "guided_samples = guided_generate(\n",
        "    rng=jax.random.PRNGKey(66),\n",
        "    num_samples=4,\n",
        "    # Note that the shape of the guidance input must be compatible with\n",
        "    # `sample[guidance_fn.slices]`\n",
        "    guidance_inputs=example_guidance_inputs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auf-UA3yGew-"
      },
      "source": [
        "Visualize guided samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72jBzmTCGEAg"
      },
      "outputs": [],
      "source": [
        "# Plot guide patch.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
        "im = ax.imshow(\n",
        "    test_example[0, 11:18, 11:18, 0] * 255, cmap=\"gray\", vmin=0, vmax=255\n",
        ")\n",
        "ax.axis(\"off\")\n",
        "ax.set_title(\"Guide patch\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot generated samples.\n",
        "fig, ax = plt.subplots(1, 4, figsize=(8, 2))\n",
        "for i in range(4):\n",
        "  im = ax[i].imshow(\n",
        "      guided_samples[i, :, :, 0] * 255, cmap=\"gray\", vmin=0, vmax=255\n",
        "  )\n",
        "  # Mark out the patch where guidance is enabled.\n",
        "  square = patches.Rectangle(\n",
        "      xy=(11, 11), width=7, height=7, fill=False, edgecolor='red'\n",
        "  )\n",
        "  ax[i].add_patch(square)\n",
        "  ax[i].axis(\"off\")\n",
        "  ax[i].set_title(f\"Sample #{i}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq4xz1w9GkFE"
      },
      "source": [
        "## Example II - Conditional diffusion model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElgwVoIPGxRq"
      },
      "source": [
        "In the above example, we trained an *unconditional* diffusion model and applied conditioning at inference time. This is not always easy to do, depending on how the conditioning input relates to the samples.\n",
        "\n",
        "Alternatively, we can directly *train a conditional model*, where the conditional signal is provided at training time as an additional input to the denoising neural network, which may then use it to compute the denoised target.\n",
        "\n",
        "Below we show an example of how to accomplish this. We again generate samples of handwritten digits, using the MNIST dataset for training. We will condition the generation on the `x[11:18, 11:18]` patch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U-O2msbGzEx"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba3Wn2YvG1oC"
      },
      "source": [
        "Besides the sample in `x`, the dataset for training conditional models require a `cond` key which contains the condition signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IpRYEJtGD-Q"
      },
      "outputs": [],
      "source": [
        "def preproc_example(example: dict[str, tf.Tensor]):\n",
        "  processed = {}\n",
        "  processed[\"x\"] = tf.cast(example[\"image\"], tf.float32) / 255.0\n",
        "\n",
        "  # The \"channel:\" prefix indicate that the conditioning signal is to be\n",
        "  # incorporated by resizing and concatenating along the channel dimension.\n",
        "  # This is implemented at the backbone level.\n",
        "  processed[\"cond\"] = {\"channel:low_res\": processed[\"x\"][11:18, 11:18]}\n",
        "  return processed\n",
        "\n",
        "\n",
        "def get_cond_mnist_dataset(split: str, batch_size: int):\n",
        "  ds = tfds.load(\"mnist\", split=split)\n",
        "  ds = ds.map(preproc_example)\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  ds = ds.as_numpy_iterator()\n",
        "  return ds\n",
        "\n",
        "DATA_STD = 0.31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yOBMiJtG7r3"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZNUY5kQG9xd"
      },
      "source": [
        "The architecture is similar to the unconditional case. We provide additional args that specify how to resize the conditioning signal (in order to be compatible with the noisy sample for channel-wise concatenation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5F8kNMAGiTR"
      },
      "outputs": [],
      "source": [
        "cond_denoiser_model = dfn_lib.PreconditionedDenoiserUNet(\n",
        "    out_channels=1,\n",
        "    num_channels=(64, 128),\n",
        "    downsample_ratio=(2, 2),\n",
        "    num_blocks=4,\n",
        "    noise_embed_dim=128,\n",
        "    padding=\"SAME\",\n",
        "    use_attention=True,\n",
        "    use_position_encoding=True,\n",
        "    num_heads=8,\n",
        "    sigma_data=DATA_STD,\n",
        "    cond_resize_method=\"cubic\",\n",
        "    cond_embed_dim=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19oJrFsjHCIZ"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT0JP9yAHEem"
      },
      "source": [
        "The `DenoisingModel` is again similar to the unconditional case. We additionally provide the shape information of the `cond` input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJFKb060GiRH"
      },
      "outputs": [],
      "source": [
        "diffusion_scheme = dfn_lib.Diffusion.create_variance_exploding(\n",
        "    sigma=dfn_lib.tangent_noise_schedule(),\n",
        "    data_std=DATA_STD,\n",
        ")\n",
        "\n",
        "cond_model = dfn.DenoisingModel(\n",
        "    input_shape=(28, 28, 1),\n",
        "    # `cond_shape` must agree with the expected structure and shape\n",
        "    # (without the batch dimension) of the `cond` input.\n",
        "    cond_shape={\"channel:low_res\": (7, 7, 1)},\n",
        "    denoiser=cond_denoiser_model,\n",
        "    noise_sampling=dfn_lib.log_uniform_sampling(\n",
        "        diffusion_scheme, clip_min=1e-4, uniform_grid=True,\n",
        "    ),\n",
        "    noise_weighting=dfn_lib.edm_weighting(data_std=DATA_STD),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81Bktr9gHHjz"
      },
      "source": [
        "The rest mostly repeats the unconditional training example, replacing the datasets and model with their conditional counterparts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyEsCCSbGiPC"
      },
      "outputs": [],
      "source": [
        "# !rm -R -f $cond_workdir  # optional: clear the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekXD8PprGiM8"
      },
      "outputs": [],
      "source": [
        "num_train_steps = 100_000  #@param\n",
        "cond_workdir = \"/tmp/cond_diffusion_demo_mnist\"  #@param\n",
        "train_batch_size = 32  #@param\n",
        "eval_batch_size = 32  #@param\n",
        "initial_lr = 0.0  #@param\n",
        "peak_lr = 1e-4  #@param\n",
        "warmup_steps = 1000  #@param\n",
        "end_lr = 1e-6  #@param\n",
        "ema_decay = 0.999  #@param\n",
        "ckpt_interval = 1000  #@param\n",
        "max_ckpt_to_keep = 5  #@param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DDpmV-zGiKW"
      },
      "outputs": [],
      "source": [
        "cond_trainer = dfn.DenoisingTrainer(\n",
        "    model=cond_model,\n",
        "    rng=jax.random.PRNGKey(888),\n",
        "    optimizer=optax.adam(\n",
        "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
        "            init_value=initial_lr,\n",
        "            peak_value=peak_lr,\n",
        "            warmup_steps=warmup_steps,\n",
        "            decay_steps=num_train_steps,\n",
        "            end_value=end_lr,\n",
        "        ),\n",
        "    ),\n",
        "    ema_decay=ema_decay,\n",
        ")\n",
        "\n",
        "templates.run_train(\n",
        "    train_dataloader=get_cond_mnist_dataset(\n",
        "        split=\"train[:75%]\", batch_size=train_batch_size\n",
        "    ),\n",
        "    trainer=cond_trainer,\n",
        "    workdir=cond_workdir,\n",
        "    total_train_steps=num_train_steps,\n",
        "    metric_writer=metric_writers.create_default_writer(\n",
        "        cond_workdir, asynchronous=False\n",
        "    ),\n",
        "    metric_aggregation_steps=100,\n",
        "    eval_dataloader=get_cond_mnist_dataset(\n",
        "        split=\"train[75%:]\", batch_size=eval_batch_size\n",
        "    ),\n",
        "    eval_every_steps = 1000,\n",
        "    num_batches_per_eval = 2,\n",
        "    callbacks=(\n",
        "        templates.TqdmProgressBar(\n",
        "            total_train_steps=num_train_steps,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "        templates.TrainStateCheckpoint(\n",
        "            base_dir=cond_workdir,\n",
        "            options=ocp.CheckpointManagerOptions(\n",
        "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojUo2JDEHPCN"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS0m_f0CHR5i"
      },
      "source": [
        "To perform inference/sampling, let's load back the trained conditional model checkpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RHlke6pGiHx"
      },
      "outputs": [],
      "source": [
        "trained_state = dfn.DenoisingModelTrainState.restore_from_orbax_ckpt(\n",
        "    f\"{cond_workdir}/checkpoints\", step=None\n",
        ")\n",
        "# Construct the inference function\n",
        "cond_denoise_fn = dfn.DenoisingTrainer.inference_fn_from_state_dict(\n",
        "    trained_state, use_ema=True, denoiser=cond_denoiser_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3crKP7hqHV7I"
      },
      "source": [
        "The conditional sampler again follows the previous example, with the only exception being that the conditional model replaces the unconditional one.\n",
        "\n",
        "Below we do not apply any guidance, but one can be easily added in the same way as in the unconditional example above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnaFzOjOHOu4"
      },
      "outputs": [],
      "source": [
        "cond_sampler = dfn_lib.SdeSampler(\n",
        "    input_shape=(28, 28, 1),\n",
        "    integrator=solver_lib.EulerMaruyama(),\n",
        "    tspan=dfn_lib.edm_noise_decay(\n",
        "        diffusion_scheme, rho=7, num_steps=256, end_sigma=1e-3,\n",
        "    ),\n",
        "    scheme=diffusion_scheme,\n",
        "    denoise_fn=cond_denoise_fn,\n",
        "    guidance_transforms=(),\n",
        "    apply_denoise_at_end=True,\n",
        "    return_full_paths=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lYF1OUEHZFM"
      },
      "source": [
        "We again JIT the generate function for the sake of faster repeated sampling calls. Here we employ `functools.partial` to specify `num_samples=5`, making it easier to vectorize across the batch dimension with `jax.vmap`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT_rLzdgHOsm"
      },
      "outputs": [],
      "source": [
        "num_samples_per_cond = 5\n",
        "\n",
        "generate = jax.jit(\n",
        "    functools.partial(cond_sampler.generate, num_samples_per_cond)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_9TfCMSHd3P"
      },
      "source": [
        "Loading a test batch of conditions with 4 elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tODWrPBfHOqN"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "test_ds = get_cond_mnist_dataset(split=\"test\", batch_size=4)\n",
        "test_batch_cond = next(iter(test_ds))[\"cond\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HJXzAtzHhli"
      },
      "source": [
        "The vectorized generate function is applied to the loaded batch. The vectorization occurs for the leading dimensions of both the random seed and the condition (for those unfamiliarized with vectorized operations in jax, think of a more efficient `for` loop that iterates over the random seeds and batch conditions zipped together)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29oPpGuWHhYP"
      },
      "outputs": [],
      "source": [
        "cond_samples = jax.vmap(generate, in_axes=(0, 0, None))(\n",
        "    jax.random.split(jax.random.PRNGKey(8888), batch_size),\n",
        "    test_batch_cond,\n",
        "    None,  # Guidance inputs = None since no guidance transforms involved\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH9CozY8Hkgv"
      },
      "source": [
        "The result `cond_samples` has shape `(batch_size, num_samples_per_cond, *input_shape)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSQHV5FmHOoK"
      },
      "outputs": [],
      "source": [
        "print(cond_samples.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br6clK2cHow9"
      },
      "source": [
        "Visualize generated examples alongside their low-res conditioning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7c5wXDcHOkP"
      },
      "outputs": [],
      "source": [
        "for i in range(batch_size):\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
        "  im = ax.imshow(\n",
        "      test_batch_cond[\"channel:low_res\"][i, :, :, 0] * 255,\n",
        "      cmap=\"gray\", vmin=0, vmax=255\n",
        "  )\n",
        "  ax.axis(\"off\")\n",
        "  ax.set_title(f\"Low-res condition: #{i + 1}\")\n",
        "\n",
        "\n",
        "  # Plot generated samples.\n",
        "  fig, ax = plt.subplots(\n",
        "      1, num_samples_per_cond, figsize=(num_samples_per_cond * 2, 2)\n",
        "  )\n",
        "  for j in range(num_samples_per_cond):\n",
        "    im = ax[j].imshow(\n",
        "        cond_samples[i, j, :, :, 0] * 255, cmap=\"gray\", vmin=0, vmax=255\n",
        "    )\n",
        "    square = patches.Rectangle(\n",
        "        xy=(11, 11), width=7, height=7, fill=False, edgecolor='red'\n",
        "    )\n",
        "    ax[j].add_patch(square)\n",
        "    ax[j].set_title(f\"conditional sample: #{j + 1}\")\n",
        "    ax[j].axis(\"off\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1eA8hF0r-tUgIX-miyPgPkzH80WjzCarp",
          "timestamp": 1707268348992
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
